{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding:UTF-8\n",
    "'''\n",
    "Author: marco lin\n",
    "Date: 2015-08-28\n",
    "'''\n",
    "\n",
    "from numpy import *\n",
    "import pickle\n",
    "import jieba\n",
    "import time\n",
    "\n",
    "stop_word = []\n",
    "'''\n",
    "    停用词集, 包含“啊，吗，嗯”一类的无实意词汇以及标点符号\n",
    "'''\n",
    "def loadStopword():\n",
    "    fr = open('stopword.txt', 'r')\n",
    "    lines = fr.readlines()\n",
    "    for line in lines:\n",
    "        stop_word.append(line.strip().decode('utf-8'))\n",
    "    fr.close()\n",
    "        \n",
    "'''\n",
    "    创建词集\n",
    "    params:\n",
    "        documentSet 为训练文档集\n",
    "    return:词集, 作为词袋空间\n",
    "'''\n",
    "def createVocabList(documentSet):\n",
    "    vocabSet = set([])\n",
    "    for document in documentSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)\n",
    "    \n",
    "'''\n",
    "    载入数据\n",
    "'''\n",
    "def loadData():\n",
    "    return None\n",
    "    \n",
    "'''\n",
    "   文本处理，如果是未处理文本，则先分词（jieba分词）,再去除停用词\n",
    "'''\n",
    "def textParse(bigString, load_from_file=True):    #input is big string, #output is word list\n",
    "    if load_from_file:\n",
    "        listOfWord = bigString.split('/ ')\n",
    "        listOfWord = [x for x in listOfWord if x != ' ']\n",
    "        return listOfWord\n",
    "    else:\n",
    "        cutted = jieba.cut(bigString, cut_all=False)\n",
    "        listOfWord  = []\n",
    "        for word in cutted:\n",
    "            if word not in stop_word:\n",
    "                listOfWord.append(word)\n",
    "        return [word.encode('utf-8') for word in listOfWord]\n",
    "        \n",
    "'''\n",
    "    交叉训练\n",
    "'''\n",
    "CLASS_AD        = 1\n",
    "CLASS_NOT_AD    = 0\n",
    "\n",
    "def testClassify():\n",
    "    listADDoc = []\n",
    "    listNotADDoc = []\n",
    "    listAllDoc = []\n",
    "    listClasses = []\n",
    "    \n",
    "    print \"----loading document list----\"\n",
    "    \n",
    "    #两千个标注为广告的文档\n",
    "    for i in range(1, 1001):\n",
    "        wordList = textParse(open('subject/subject_ad/%d.txt' % i).read())\n",
    "        listAllDoc.append(wordList)\n",
    "        listClasses.append(CLASS_AD)\n",
    "    #两千个标注为非广告的文档\n",
    "    for i in range(1, 1001):\n",
    "        wordList = textParse(open('subject/subject_notad/%d.txt' % i).read())\n",
    "        listAllDoc.append(wordList)\n",
    "        listClasses.append(CLASS_NOT_AD)\n",
    "    \n",
    "    print \"----creating vocab list----\"    \n",
    "    #构建词袋模型\n",
    "    listVocab = createVocabList(listAllDoc)\n",
    "    \n",
    "    docNum = len(listAllDoc)\n",
    "    testSetNum  = int(docNum * 0.1);\n",
    "    \n",
    "    trainingIndexSet = range(docNum)   # 建立与所有文档等长的空数据集（索引）\n",
    "    testSet = []                       # 空测试集\n",
    "    \n",
    "    # 随机索引，用作测试集, 同时将随机的索引从训练集中剔除\n",
    "    for i in range(testSetNum):\n",
    "        randIndex = int(random.uniform(0, len(trainingIndexSet)))\n",
    "        testSet.append(trainingIndexSet[randIndex])\n",
    "        del(trainingIndexSet[randIndex])\n",
    "    \n",
    "    trainMatrix = []\n",
    "    trainClasses = []\n",
    "   \n",
    "    for docIndex in trainingIndexSet:\n",
    "        trainMatrix.append(bagOfWords2VecMN(listVocab, listAllDoc[docIndex]))\n",
    "        trainClasses.append(listClasses[docIndex])\n",
    "    \n",
    "    print \"----traning begin----\"\n",
    "    pADV, pNotADV, pClassAD = trainNaiveBayes(array(trainMatrix), array(trainClasses))\n",
    "    \n",
    "    print \"----traning complete----\"\n",
    "    print \"pADV:\", pADV\n",
    "    print \"pNotADV:\", pNotADV\n",
    "    print \"pClassAD:\", pClassAD\n",
    "    print \"ad: %d, not ad:%d\" % (CLASS_AD, CLASS_NOT_AD)\n",
    "    \n",
    "    args = dict()\n",
    "    args['pADV'] = pADV\n",
    "    args['pNotADV'] = pNotADV\n",
    "    args['pClassAD'] = pClassAD\n",
    "    \n",
    "    fw = open(\"args.pkl\", \"wb\")\n",
    "    pickle.dump(args, fw, 2)\n",
    "    fw.close()\n",
    "    \n",
    "    fw = open(\"vocab.pkl\", \"wb\")\n",
    "    pickle.dump(listVocab, fw, 2)\n",
    "    fw.close()\n",
    "\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        vecWord = bagOfWords2VecMN(listVocab, listAllDoc[docIndex])\n",
    "        if classifyNaiveBayes(array(vecWord), pADV, pNotADV, pClassAD) != listClasses[docIndex]:\n",
    "            errorCount += 1\n",
    "            doc = ' '.join(listAllDoc[docIndex])\n",
    "            print \"classfication error\", doc.decode('utf-8', \"ignore\").encode('gbk')\n",
    "    print 'the error rate is: ', float(errorCount) / len(testSet)\n",
    "        \n",
    "# 分类方法(这边只做二类处理)\n",
    "def classifyNaiveBayes(vec2Classify, pADVec, pNotADVec, pClass1):\n",
    "    pIsAD = sum(vec2Classify * pADVec) + log(pClass1)    #element-wise mult\n",
    "    pIsNotAD = sum(vec2Classify * pNotADVec) + log(1.0 - pClass1)\n",
    "    \n",
    "    if pIsAD > pIsNotAD:\n",
    "        return CLASS_AD\n",
    "    else: \n",
    "        return CLASS_NOT_AD\n",
    "    \n",
    "'''\n",
    "    训练\n",
    "    params:\n",
    "        tranMatrix 由测试文档转化成的词空间向量 所组成的 测试矩阵\n",
    "        tranClasses 上述测试文档对应的分类标签\n",
    "'''\n",
    "def trainNaiveBayes(trainMatrix, trainClasses):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0]) #计算矩阵列数, 等于每个向量的维数\n",
    "    numIsAD  = len(filter(lambda x: x == CLASS_AD, trainClasses))\n",
    "    pClassAD = numIsAD / float(numTrainDocs)\n",
    "    pADNum = ones(numWords); pNotADNum = ones(numWords)\n",
    "    pADDenom = 2.0; pNotADDenom = 2.0\n",
    "    \n",
    "    for i in range(numTrainDocs):\n",
    "        if trainClasses[i] == CLASS_AD:\n",
    "            pADNum += trainMatrix[i]\n",
    "            pADDenom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            pNotADNum += trainMatrix[i]\n",
    "            pNotADDenom += sum(trainMatrix[i])\n",
    "        \n",
    "    pADVect = log(pADNum / pADDenom)\n",
    "    pNotADVect = log(pNotADNum / pNotADDenom)\n",
    "    \n",
    "    return pADVect, pNotADVect, pClassAD\n",
    "    \n",
    "'''\n",
    "    将输入转化为向量，其所在空间维度为 len(listVocab)\n",
    "    params: \n",
    "        listVocab-词集\n",
    "        inputSet-分词后的文本，存储于set\n",
    "'''\n",
    "def bagOfWords2VecMN(listVocab, inputSet):\n",
    "    returnVec = [0]*len(listVocab)\n",
    "    for word in inputSet:\n",
    "        if word in listVocab:\n",
    "            returnVec[listVocab.index(word)] += 1\n",
    "    return returnVec\n",
    "    \n",
    "'''\n",
    "    读取保存的模型，做分类操作\n",
    "'''\n",
    "def adClassify(text):\n",
    "    fr = open(\"args.pkl\", \"rb\")\n",
    "    args = pickle.load(fr)\n",
    "    pADV        = args['pADV']\n",
    "    pNotADV     = args['pNotADV']\n",
    "    pClassAD    = args['pClassAD']\n",
    "    fr.close()\n",
    "\n",
    "    fr = open(\"vocab.pkl\", \"rb\")\n",
    "    listVocab = pickle.load(fr)\n",
    "    fr.close()\n",
    "    \n",
    "    if len(listVocab) == 0:\n",
    "        print \"got no args\"\n",
    "        return\n",
    "        \n",
    "    text = textParse(text, False)\n",
    "    vecWord = bagOfWords2VecMN(listVocab, text)\n",
    "    class_type = classifyNaiveBayes(array(vecWord), pADV, pNotADV, pClassAD)\n",
    "        \n",
    "    print \"classfication type:%d\" % class_type\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    loadStopword()\n",
    "    while True:\n",
    "        opcode = raw_input(\"input 1 for training, 2 for ad classify: \")\n",
    "        if opcode.strip() == \"1\":\n",
    "            begtime = time.time()\n",
    "            testClassify()\n",
    "            print \"cost time total:\", time.time() - begtime\n",
    "        else:\n",
    "            text = raw_input(\"input the text:\")\n",
    "            adClassify(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
